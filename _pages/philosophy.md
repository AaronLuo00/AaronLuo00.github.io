---
layout: default
title: "My Philosophy"
permalink: /philosophy
author_profile: true
---

# My Philosophy

## The Power of Pattern Recognition and Interdisciplinary Thinking

From an early age, I've been fascinated by patterns and underlying principles that govern different aspects of our world. Like a detective piecing together clues, I find joy in discovering how things work, whether it's the explicit rules of a system or the implicit dynamics within communities.

This curiosity has led me to explore far beyond my primary field of AI, venturing into diverse areas such as cryptocurrency, politics, finance, and psychology. I believe that reality is multifaceted, and each discipline offers a unique lens through which to view and understand it. Just as a diamond's beauty is revealed through its many facets, true understanding comes from examining issues from multiple perspectives.

It was this same pattern-seeking instinct that drew me to healthcare AI — a field where the patterns are complex, the stakes are real, and the gap between what's technically possible and what's practically useful remains wide open.

## Three Principles

Across every domain I've explored, three ideas keep surfacing:

### 1. Inputs → Outputs

I believe in causality. Good systems, sustained effort, and honest feedback produce good outcomes — not luck, not talent alone. This applies to research, to health, to relationships. If the output isn't what I want, I look at the inputs before I look for excuses. Compound growth is real, but only if you're compounding the right things.

### 2. Build, Don't Browse

The most valuable work is deep, not wide. I'd rather spend a thousand hours on one problem that matters than a hundred hours each on ten that don't. In an age of infinite information, the scarce resource isn't knowledge — it's the discipline to go deep on a single hard question and stay there until something real emerges.

### 3. Stand at the Intersection

The most interesting problems live at the boundaries between fields. I chose to stand at the intersection of AI and healthcare — not because it's trendy, but because it demands both technical rigor and genuine understanding of human stakes. Building AI that works in a lab is an engineering problem. Building AI that works in a hospital is a human problem.

## What I'm Thinking About

These are the questions I'm currently wrestling with in my research:

- **What happens when doctors start depending on AI — and it's wrong?** As clinical AI tools become more capable, the risk shifts from "AI isn't good enough" to "humans trust AI too much." How do we design systems that keep clinicians sharp, not complacent?

- **How do we measure AI failures that don't show up in benchmarks?** A model can score 95% on a test set and still fail catastrophically in deployment. I'm interested in evaluation frameworks that capture real-world failure modes — the ones that matter to patients.

- **Can AI systems know what they don't know?** Uncertainty estimation and graceful degradation aren't just nice features — in healthcare, they're the difference between a helpful tool and a dangerous one.

## Where I'm Going

I want to build AI systems that clinicians trust with their patients' lives — and that earn that trust through reliability, transparency, and honest uncertainty. This is a hard problem. It requires years, not months. I'm willing to spend them.

The same pattern recognition that once led me across disciplines now keeps me focused on one: making AI work where it matters most. The world doesn't need more papers that push a leaderboard number. It needs systems that work in the messy, high-stakes reality of clinical practice.

---

*Always happy to talk about these ideas. Reach me at [xiaolongluo@g.harvard.edu](mailto:xiaolongluo@g.harvard.edu).*
